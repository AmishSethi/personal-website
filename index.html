<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Amish Sethi</title>
  <meta name="author" content="Amish Sethi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <!-- Intro Section -->
        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p class="name" style="text-align: center;">Amish Sethi</p>
              <p>
                I'm an undergraduate student at the <a href="https://www.seas.upenn.edu/">University of
                  Pennsylvania</a>, currently in my senior year pursuing a Bachelor's of Science in Engineering with a
                4.0/4.0 GPA.
                My major is Computer&nbsp;Science, and I am getting minors in mathematics and data science, with an
                expected graduation in May&nbsp;2026.
              </p>
              <p>
                I work with <a href="https://www.linkedin.com/in/ai4code/">Professor&nbsp;Mayur&nbsp;Naik</a> and
                <a href="https://www.seas.upenn.edu/~dineshj/" target="_blank">Professor&nbsp;Dinesh&nbsp;Jayaraman</a>
                on scalable neurosymbolic learning, robust video perception, and embodied AI. My interests span
                neural-network optimization, structured and trustworthy perception for robots, and foundation models
                that connect videos, language, and robot policies. I enjoy building efficient, interpretable systems
                that make large language and vision models reliable in the physical world.
              </p>
              <p>
                I'm incredibly grateful to
                <a href="https://www.linkedin.com/in/ai4code/">Professor Mayur Naik</a> and
                <a href="https://www.seas.upenn.edu/~dineshj/" target="_blank">Professor Dinesh Jayaraman</a> for
                their continuous support and guidance throughout my research journey. I also deeply appreciate the
                mentorship and inspiration from the PhD students I've worked closely with:
                <a href="https://nvelingker.github.io" target="_blank">Neelay Velingker</a>,
                <a href="https://oscarxzq.github.io" target="_blank">Oscar Xu</a>,
                <a href="https://www.seas.upenn.edu/~asnaik/" target="_blank">Aaditya Naik</a>, and
                <a href="https://www.cis.upenn.edu/~jianih/" target="_blank">Jiani Huang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:asethi04@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=k7JSR2UAAAAJ">Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/AmishSethi">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:37%;max-width:37%">
              <a href="images/new_amish.jpg">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                  src="images/new_amish.jpg" class="hoverZoomLink">
              </a>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:auto;margin-top:-10px;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul class="news-list">
                <li>(2025/12/17) I received an <strong>Honorable Mention</strong> for the
                  <a href="https://cra.org/crae/awards/cra-outstanding-undergraduate-researchers/" target="_blank">
                    CRA Outstanding Undergraduate Researcher Award</a>. ðŸŽ‰
                </li>
                <li>(2025/12/15) Google highlighted our ESCA work on the
                  <a href="https://opensource.googleblog.com/2025/12/grounding-embodied-agents-with-scene-graphs-accelerated-by-jax.html"
                    target="_blank">
                    Google Open Source Blog</a>, showcasing how we used JAX to achieve real-time scene graph generation
                  for embodied AI.
                  ðŸŽ‰
                </li>
                <li>(2025/12/03) Penn Engineering highlighted our NeurIPS 2025 Spotlight paper

                  ESCA: Contextualizing Embodied Agents via Scene-Graph Generation
                  on their <a
                    href="https://www.linkedin.com/posts/pennengai_neurips-2025-research-spotlight-esca-contextualizing-activity-7402112890218004480-vJSv"
                    target="_blank"> <em>socials</em></a> and the front page of their website. ðŸŽ‰
                </li>
                <li>(2025/12/02) The <a href="https://github.com/video-fm/LASER" target="_blank"> <em>GitHub
                      repo</em></a> behind our foundation model in ESCA reached over <strong>100 stars</strong> ðŸŽ‰
                </li>
                <li>(2025/11/01) I was selected as one of Penn's 4 nominees for the
                  <strong>CRA Outstanding Undergraduate Researcher Award</strong>. ðŸŽ‰
                </li>
                <li>(2025/09/17) Our paper
                  <a href="esca.pdf" target="_blank">
                    <em>ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</em></a>
                  was accepted to <strong>NeurIPS 2025</strong> as a <strong>Spotlight</strong> (top 3%)! ðŸŽ‰
                </li>
                <li>(2025/05/01) Our paper
                  <a href="https://arxiv.org/pdf/2410.03348" target="_blank">
                    <em>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</em></a>
                  was accepted to <strong>ICML 2025</strong>! ðŸŽ‰
                </li>
              </ul>
            </td>
          </tr>
        </table>

        <!-- Research Summary -->
        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in deep learning and neurosymbolic methods for building <strong>reliable embodied
                  AI</strong>. My work spans three layers of the AI stack: (1) optimizing and understanding large
                language
                models, (2) scalable neurosymbolic frameworks, and (3) structured video and robot policy models that
                make perception and action robust in open-ended environments.
              </p>
            </td>
          </tr>
        </table>

        <!-- Project Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;">

          <!-- Project 0: Retrieval-Augmented VLA (RSS thesis) -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/vla.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Retrieval-Augmented Vision-Language-Action Policies from Broad Robot Demonstration
                Corpora</papertitle><br>
              <strong><em>Amish Sethi</em></strong>, Jiani Huang, Felix Zheng, Brandon Yang, Chris Watson, Aurora Qian,
              Junyao Shi, Mayur Naik, Dinesh Jayaraman<br>
              <em>Robotics: Science and Systems (RSS) 2026 &mdash; planned submission (senior thesis)</em><br><br>
              <p>
                This project builds retrieval-augmented vision-language-action (VLA) policies that adapt to new
                manipulation tasks without additional in-domain teleoperation. We use VINEâ€™s spatio-temporal
                scene graphs to index large corpora of robot manipulation trajectories and retrieve structurally similar
                examples at inference time, enabling a Franka arm to solve novel multi-stage tasks via in-context
                learning alone (no finetuning or additional data collection needed).
              </p>
            </td>
          </tr>

          <!-- Project 2: ESCA/VINE -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/vine_pipeline_final.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>ESCA: Contextualizing Embodied Agents via Scene-Graph Generation</papertitle><br>
              Jiani Huang<sup>*</sup>, <strong><em>Amish Sethi<sup>*</sup></em></strong><sup>*</sup>, Matthew
              Kuo<sup>*</sup>, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ziyang Li, Ser-Nam Lim, Mayur Naik<br>
              <em>NeurIPS 2025 <strong>Spotlight</strong> (top 3%)</em><br>
              <a href="esca.pdf">paper</a> /
              <a href="https://video-fm.github.io">website</a>
              <p>ESCA addresses that up to 69% of embodied AI failures stem from perception errors. Using VINE, a
                foundation model that extracts spatio-temporal scene graphs from video, ESCA provides explicit spatial
                context for vision-language models. Our approach improved success rates by up to 10%, spatial reasoning
                by 14.6%, and reduced perception errors from 69% to 30% on EmbodiedBench without requiring model
                retraining.</p>
            </td>
          </tr>

          <!-- Project 3: VINE -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/vine.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>VINE: A Foundation Model for Video Understanding</papertitle><br>
              <strong><em>Amish Sethi<sup>*</sup></em></strong>, Jiani Huang<sup>*</sup>, Matthew Kuo<sup>*</sup>,
              Ziyang Li, Mayank Keoliya, Neelay Velingker, Mayur Naik, Sernam Lim<br>
              <em>Foundation Model</em><br>
              <a href="https://video-fm.github.io">website</a><br><br>
              <a href="https://github.com/video-fm/LASER_v0">code</a> /
              <a href="https://huggingface.co/video-fm/vine_v0">model</a> /
              <a href="https://huggingface.co/datasets/video-fm/ESCA-video-87K">dataset</a><br><br>
              <p>VINE is a foundation model that transforms video into structured scene graphs capturing entities,
                attributes, spatial relationships, and temporal dynamics. Given a video and optional keywords, VINE
                outputs probabilistic scene graphs that provide rich semantic structure beyond object detection. Trained
                on 87K+ videos using neurosymbolic learning, VINE is both promptable and fine-tuneable for diverse
                applications from contextualizing vision-language models to learning policies from broad robot
                demonstration corpora.</p>
            </td>
          </tr>

          <!-- Project 1: Delta Activations -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/delta.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Delta Activations: A Representation for Finetuned Large Language Models</papertitle><br>
              Zhiqiu Xu<sup>*</sup>, <strong><em>Amish Sethi</em></strong><sup>*</sup>, Mayur Naik, Ser-Nam Lim<br>
              <em>NeurIPS 2025 ER Workshop</em><br>
              <a href="https://arxiv.org/abs/2509.04442">arXiv</a> /
              <a href="https://oscarxzq.github.io/delta_activation/">website</a><br><br>
              <p>Delta Activations represents finetuned models by measuring shifts in their internal activations
                relative to a base model. This approach clusters models by domain and enables retrieval using only 20
                examples. We finetuned and released over 700 open-source models on Hugging Face, demonstrating the
                utility of this representation for model selection and merging in building reliable model ecosystems.
              </p>
            </td>
          </tr>

          <!-- Project 4: Dolphin -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/dolphin.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</papertitle><br>
              Aaditya Naik, Jason Liu, Claire Wang, <strong><em>Amish Sethi</em></strong>, Saikat Dutta, Mayur Naik,
              Eric Wong<br>
              <em>ICML 2025</em><br>
              <a href="https://arxiv.org/pdf/2410.03348">Publication</a><br><br>
              <p>DOLPHIN is a novel framework combining symbolic reasoning and neural computation using CPU-GPU hybrid
                execution. Its execution of vectorized probabilistic computations on the GPU allows it to achieve up to
                62Ã— faster convergence than baselines across 13 benchmarks spanning text, image, and video modalities.
              </p>
            </td>
          </tr>

          <!-- Project 5: CLAM -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/clam.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>CLAM: Unifying Finetuning, Quantization, and Pruning by Chaining LLM Adapter Modules
              </papertitle><br>
              Neelay Velingker, <strong><em>Amish Sethi</em></strong><sup>*</sup>, Jason Liu<sup>*</sup>, William
              Dodds<sup>*</sup>, Zhiqiu Xu, Saikat Dutta, Mayur Naik, Eric Wong<br>
              <em>ICML ES-FoMo II 2024</em><br>
              <a href="https://openreview.net/pdf?id=1mKtFkElnu">paper</a> /
              <a href="https://github.com/slimscale-ai/slimscale">code</a><br><br>
              <p>CLAM is a framework unifying parameter-efficient finetuning, quantization, and pruning for LLMs. It
                enables chaining of adapters with low overhead and high modularity, outperforming state-of-the-art
                methods by up to 6.5%. CLAM achieves superior trade-offs in compression and downstream performance,
                beating QLoRA while effectively halving the number of active bits.</p>
            </td>
          </tr>

          <!-- Project 6: Alzheimer's -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/alz.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Functional Genetic Biomarkers of Alzheimer's Disease and Gene Expression from Peripheral Blood
              </papertitle><br>
              <strong>Amish Sethi</strong><sup>*</sup>, Andrew Ni<sup>*</sup><br>
              <em>International Science and Engineering Fair 2021</em><br>
              <a href="https://www.biorxiv.org/content/10.1101/2021.01.15.426891v1.full.pdf">paper</a><br><br>
              <p>This project utilized machine learning, clustering, and dimensionality reduction algorithms in
                <code>scikit-learn</code> to identify which genes are expressed differently between those with
                Alzheimer's and a control group. A model trained on this gene expression data could predict likelihood
                of Alzheimer's with 98% accuracy.
              </p>
              <span style="font-size: 90%; color: gray;">Cited over 8 times and viewed over 1,000 times on
                biorxiv.</span>
            </td>
          </tr>

          <!-- Equal contribution note -->
          <tr>
            <td colspan="2" style="padding:20px;padding-top:10px;">
              <p style="font-size: 85%; color: #666; margin:0;"><sup>*</sup> Equal contribution</p>
            </td>
          </tr>
        </table>

        <!-- Teaching Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;margin-top:40px;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Teaching and Mentorship</h2>
              <p>
                In the <strong>Fall of 2024</strong>, I served as the Head Teaching Assistant (TA) for <a
                  href="https://llm-class.github.io" target="_blank">CIS 7000: Large Language Models</a>, the University
                of Pennsylvania's first dedicated course on LLMs.
                The course enrolled over <strong>120 students</strong> and covered the theory, design, training,
                compression, deployment, and application of large language models.
              </p>
              <p>
                As Head TA, I was responsible for:
              <ul>
                <li>Planning the course curriculum</li>
                <li>Designing and implementing homework assignments</li>
                <li>Holding office hours and supporting students throughout the semester</li>
                <li>Creating several lecture slide decks</li>
                <li>Delivering lectures on efficient finetuning, adaptation, and evaluation</li>
              </ul>
              The course received a <strong>TA quality rating of 3.15</strong> and an <strong>overall course quality
                rating of 3.01</strong> out of 4.
              </p>

              <br>
              <p>
                In the <strong>Summer of 2024</strong>, I mentored five undergraduate students through the
                <a href="https://curf.upenn.edu/content/penn-undergraduate-research-mentoring-program-purm"
                  target="_blank">Penn Undergraduate Research Mentoring Program (PURM)</a>
                on the CLAM project, focusing on efficient finetuning, quantization, and pruning.
              </p>

              <p>
                I taught these students how to conduct research in machine learning, work with LLMs, and develop
                scalable optimization frameworks. The students I mentored were:
              <ul>
                <li><a href="https://www.linkedin.com/in/annabellatian/" target="_blank">Annabella Tian</a></li>
                <li><a href="https://www.linkedin.com/in/cedricwchan/" target="_blank">Cedric Chan</a></li>
                <li><a href="https://www.linkedin.com/in/ani-petrosiann/" target="_blank">Ani Petrosian</a></li>
                <li><a href="https://www.linkedin.com/in/andrew-chang-09654228a/" target="_blank">Andrew Chang</a></li>
                <li><a href="https://www.linkedin.com/in/kuomat/" target="_blank">Matthew Kuo</a></li>
              </ul>
              </p>
            </td>
          </tr>
        </table>

        <!-- Academic Services Section -->
    <tr>
      <td style="padding:16px;width:100%;vertical-align:middle">
        <h2>Academic Services</h2>
        <p>
          I served as a reviewer for the <a href="https://icml.cc/virtual/2024/workshop/29965" target="_blank">
            <em>Workshop on Efficient Systems for Foundation Models II</em> at ICML 2024</a>, <a
            href="https://aaai.org/conference/aaai/aaai-26/" target="_blank">AAAI 2026</a>, and <a
            href="https://iclr.cc/Conferences/2026" target="_blank">ICLR 2026</a>.
        </p>
      </td>
    </tr>

    <!-- Grant Writing Section -->
    <tr>
      <td style="padding:16px;width:100%;vertical-align:middle">
        <h2>Grant Writing</h2>
        <p>
          I authored a successful proposal for the <a
            href="https://curf.upenn.edu/content/grants-faculty-mentoring-undergraduate-research" target="_blank">
            Grant for Faculty Mentoring Undergraduate Research (GfFMUR)</a>, awarded by the University of
          Pennsylvania.
          This competitive grant supported undergraduate research efforts and resulted in an <strong>$8,000
            award</strong> to fund mentorship and research on neurosymbolic AI.
        </p>
      </td>
    </tr>

    </td>
    </tr>
  </table>
  <p style="font-size:12px; text-align:center; color:gray; margin-top:50px;">
    Website design from <a href="https://jonbarron.info" style="color:gray;" target="_blank">Jon Barron</a>, borrowed
    from <a href="https://ruiqizhong.github.io/" style="color:gray;" target="_blank">Ruiqi Zhong</a>.
  </p>

</body>

</html>