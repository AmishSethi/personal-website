<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Amish Sethi</title>
  <meta name="author" content="Amish Sethi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <!-- Intro Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p class="name" style="text-align: center;">Amish Sethi</p>
              <p>
                I'm an undergraduate student at the <a href="https://www.seas.upenn.edu/">University of Pennsylvania</a>, currently in my junior year pursuing a Bachelor's and Master's degree in Computer&nbsp;Science, with an expected graduation for both in&nbsp;2026.
              </p>
              <p>
                I work with <a href="https://www.linkedin.com/in/ai4code/">Professor&nbsp;Mayur&nbsp;Naik</a> on projects that push the boundaries of deep learning and neurosymbolic AI. My interests span neural-network optimization, scalable model compression, and integrating symbolic reasoning with neural architectures. I enjoy building efficient, interpretable systems that extend the capabilities of large language and vision models.
              </p>
              <p>
                Iâ€™m incredibly grateful to <a href="https://www.linkedin.com/in/ai4code/">Professor Mayur Naik</a> for his continuous support and guidance throughout my research journey. I also deeply appreciate the mentorship and inspiration from the PhD students Iâ€™ve worked closely with: 
                <a href="https://nvelingker.github.io" target="_blank">Neelay Velingker</a>, 
                <a href="https://oscarxzq.github.io" target="_blank">Oscar Xu</a>, 
                <a href="https://www.seas.upenn.edu/~asnaik/" target="_blank">Aaditya Naik</a>, and 
                <a href="https://www.cis.upenn.edu/~jianih/" target="_blank">Jiani Huang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:asethi04@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=k7JSR2UAAAAJ">Scholar</a> &nbsp;/&nbsp;
                <a href="https://github.com/AmishSethi">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:37%;max-width:37%">
              <a href="images/amish.jpg">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/amish.jpg" class="hoverZoomLink">
              </a>
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin:auto;margin-top:-10px;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <ul class="news-list">
                <li>(2025/05/01) Our paper <a href="https://arxiv.org/pdf/2410.03348" target="_blank">
                    <em>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</em></a>
                    is accepted to <strong>ICML 2025</strong>! ðŸŽ‰</li>
                <!-- ----- Optional older items for continuity -----
                <li>(2024/01/16) Our paper <a href="#">Initializing Models with Larger Ones</a>
                    was accepted to ICLR 2024 as a Spotlight presentation!</li>
                <li>(2023/04/24) Our paper <a href="#">Dropout Reduces Underfitting</a>
                    was accepted to ICML 2023!</li>
                <li>(2023/03/31) I will join the University of Pennsylvania as a PhD student (Fall 2023).</li>
                -->
              </ul>
            </td>
          </tr>
        </table>


        <!-- Research Summary -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in deep learning, generative AI, and neurosymbolic AI. Most of my research focuses on optimizing large language models through efficient finetuning, quantization, and pruning, and on exploring how symbolic reasoning can be woven into neural networks for greater interpretability and control. 
              </p>
            </td>
          </tr>
        </table>

        <!-- Project Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;">
          <!-- Project 1: Dolphin -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/dolphin.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</papertitle><br>
              Aaditya Naik, Jason Liu, Claire Wang, <strong><em>Amish Sethi</em></strong>, Saikat Dutta, Mayur Naik, Eric Wong<br>
              <em>ICML 2025 </em> <br>
              <a href="https://arxiv.org/pdf/2410.03348">arXiv</a><br><br>
              <p>DOLPHIN is a novel framework combining symbolic reasoning and neural computation using CPU-GPU hybrid execution. Its execution of vectorized probabilistic computations on the GPU allows it to achieve up to 62Ã— faster convergence than baselines across 13 benchmarks spanning text, image, and video modalities.</p>
            </td>
          </tr>

          <!-- Project 2: CLAM -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/clam.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>CLAM: Unifying Finetuning, Quantization, and Pruning by Chaining LLM Adapter Modules</papertitle><br>
              Neelay Velingker, <strong><em>Amish Sethi</em></strong>, Jason Liu, William Dodds, Zhiqiu Xu, Saikat Dutta, Mayur Naik, Eric Wong<br>
              <em>Workshop on Efficient Systems for Foundation Models II @ ICML 2024</em><br>
              <a href="https://openreview.net/pdf?id=1mKtFkElnu">paper</a> / 
              <a href="https://github.com/slimscale-ai/slimscale">code</a><br><br>
              <p>CLAM is a framework unifying parameter-efficient finetuning, quantization, and pruning for LLMs. It enables chaining of adapters with low overhead and high modularity, outperforming state-of-the-art methods by up to 6.5%. CLAM achieves superior trade-offs in compression and downstream performance, beating QLoRA while effectvely halving the number of active bits</p>
            </td>
          </tr>

          <!-- Project 3: LoRA -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/lora.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Activations Arenâ€™t Cheap in LoRA, Weights Are</papertitle><br>
              Neelay Velingker, Zhiqiu Xu, <strong><em>Amish Sethi</em></strong>, William Dodds, Mayur Naik<br>
              <em>ICLR 2025 Submission</em><br>
              <a href="https://openreview.net/pdf?id=3ylNuZXtMg">paper</a><br><br>
              <p>We provide a semantically-equivalent computation graph reformulation for LoRA and other PeFT techniques that saves memory and accelerates training. Under practical conditions, this leads to up to a 1.4Ã— reduction in max memory usage and latency for LoRA finetuning across language and diffusion transformers, without degrading predictive performance.</p>
            </td>
          </tr>

          <!-- Project 4: Alzheimerâ€™s -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/alz.png" alt="project image" style="width:100%; border-radius:10px;">
            </td>
            <td style="padding:20px;width:65%;vertical-align:middle">
              <papertitle>Functional Genetic Biomarkers of Alzheimerâ€™s Disease and Gene Expression from Peripheral Blood</papertitle><br>
              <strong>Andrew Ni</strong><sup>*</sup>, <strong>Amish Sethi</strong><sup>*</sup> (equal contribution)<br>
              <em>International Science and Engineering Fair 2020</em><br>
              <a href="https://www.biorxiv.org/content/10.1101/2021.01.15.426891v1.full.pdf">paper</a><br><br>
              <p>This project utilized machine learning, clustering, and dimensionality reduction algorithms in <code>scikit-learn</code> to identify which genes are expressed differently between those with Alzheimerâ€™s and a control group. A model trained on this gene expression data could predict likelihood of Alzheimerâ€™s with 98% accuracy.</p>
              <span style="font-size: 90%; color: gray;">Cited over 6 times and viewed over 1,000 times on biorxiv.</span>
            </td>
          </tr>
        </table>
        <!-- Teaching Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;margin-top:40px;">
          <tr>
            <td style="padding:16px;width:100%;vertical-align:middle">
              <h2>Teaching and Mentorship</h2>
              <p>
                In the <strong>Fall of 2024</strong>, I served as the Head Teaching Assistant (TA) for <a href="https://llm-class.github.io" target="_blank">CIS 7000: Large Language Models</a>, the University of Pennsylvaniaâ€™s first dedicated course on LLMs. 
                The course enrolled over <strong>120 students</strong> and covered the theory, design, training, compression, deployment, and application of large language models.
              </p>
              <p>
                As Head TA, I was responsible for:
                <ul>
                  <li>Planning the course cirriculum</li>
                  <li>Designing and implementing homework assignments</li>
                  <li>Holding office hours and supporting students throughout the semester</li>
                  <li>Creating several lecture slide decks</li>
                  <li>Delivering some lectures on efficient finetuning, adaptation, and evaluation</li>
                </ul>
                The course received a <strong>TA quality rating of 3.15</strong> and an <strong>overall course quality rating of 3.01</strong> out of 4.
              </p>

              <br>
              <p>
                In the <strong>Summer of 2024</strong>, I mentored five undergraduate students through the 
                <a href="https://curf.upenn.edu/content/penn-undergraduate-research-mentoring-program-purm" target="_blank">Penn Undergraduate Research Mentoring Program (PURM)</a> 
                on the CLAM project, focusing on efficient finetuning, quantization, and pruning.
              </p>
        
              <p>
                I taught these students how to conduct research in machine learning, work with LLMs, and develop scalable optimization frameworks. The students I mentored were:
                <ul>
                  <li><a href="https://www.linkedin.com/in/annabellatian/" target="_blank">Annabella Tian</a></li>
                  <li><a href="https://www.linkedin.com/in/cedricwchan/" target="_blank">Cedric Chan</a></li>
                  <li><a href="https://www.linkedin.com/in/ani-petrosiann/" target="_blank">Ani Petrosian</a></li>
                  <li><a href="https://www.linkedin.com/in/andrew-chang-09654228a/" target="_blank">Andrew Chang</a></li>
                  <li><a href="https://www.linkedin.com/in/kuomat/" target="_blank">Matthew Kuo</a></li>
                </ul>
              </p>
            </td>
          </tr>
        </table>
        <!-- Academic Services Section -->
        <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Academic Services</h2>
            <p>
              I served as a reviewer for the <a href="https://icml.cc/virtual/2024/workshop/29965" target="_blank">
              <em>Workshop on Efficient Systems for Foundation Models II</em> at ICML 2024</a>.
            </p>
          </td>
        </tr>
        <!-- Grant Writing Section -->
        <tr>
          <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Grant Writing</h2>
            <p>
              I authored a successful proposal for the <a href="https://curf.upenn.edu/content/grants-faculty-mentoring-undergraduate-research" target="_blank">
              Grant for Faculty Mentoring Undergraduate Research (GfFMUR)</a>, awarded by the University of Pennsylvania. This competitive grant supported undergraduate research efforts and resulted in an $8,000 award to fund mentorship and research on neurosymbolic AI.
            </p>
          </td>
        </tr>

      </td>
    </tr>
  </table>
  <p style="font-size:12px; text-align:center; color:gray; margin-top:50px;">
    Website design from <a href="https://jonbarron.info" style="color:gray;" target="_blank">Jon Barron</a>, borrowed from <a href="https://ruiqizhong.github.io/" style="color:gray;" target="_blank">Ruiqi Zhong</a>.
  </p>
  
</body>
</html>
